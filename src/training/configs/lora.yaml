# Example LoRA config
r: 16
lora_alpha: 16
lora_dropout: 0.0
target_modules:
  - q_proj
  - k_proj
  - v_proj
  - o_proj
  - gate_proj
  - up_proj
  - down_proj
bias: none
use_gradient_checkpointing: unsloth
use_rslora: false
random_state: 3407
