#!/usr/bin/env python3
"""
Enhanced Chess Inference Engine

Advanced inference system with:
- Simplified and optimized decoding strategies
- Intelligent caching with position awareness
- Robust expert mode switching
- Real-time performance monitoring
- Chess-specific optimizations

Addresses the core inference issues identified in the audit.
"""

import os
import sys
import time
import hashlib
import json
from pathlib import Path
from typing import Dict, List, Any, Optional, Tuple, Union
from dataclasses import dataclass, field
from collections import OrderedDict, defaultdict
import logging
import threading
from concurrent.futures import ThreadPoolExecutor
import torch
from transformers import AutoTokenizer, AutoModelForCausalLM, GenerationConfig
from peft import PeftModel
import chess

# Add project root to path
project_root = Path(__file__).resolve().parents[2]
sys.path.insert(0, str(project_root))

# Configure logging
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)


@dataclass
class InferenceConfig:
    """Configuration for enhanced inference."""
    max_new_tokens: int = 200
    temperature: float = 0.7
    top_p: float = 0.9
    top_k: int = 50
    repetition_penalty: float = 1.1
    do_sample: bool = True
    pad_token_id: Optional[int] = None
    eos_token_id: Optional[int] = None
    cache_enabled: bool = True
    cache_max_size: int = 1000
    chess_aware_decoding: bool = True
    expert_switching_enabled: bool = True
    performance_monitoring: bool = True


@dataclass
class CacheEntry:
    """Smart cache entry with position awareness."""
    key: str
    response: str
    fen: Optional[str] = None
    move_uci: Optional[str] = None
    timestamp: float = field(default_factory=time.time)
    access_count: int = 0
    expert_type: str = "default"
    confidence_score: float = 0.0

    def to_dict(self) -> Dict[str, Any]:
        return {
            'key': self.key,
            'response': self.response,
            'fen': self.fen,
            'move_uci': self.move_uci,
            'timestamp': self.timestamp,
            'access_count': self.access_count,
            'expert_type': self.expert_type,
            'confidence_score': self.confidence_score
        }


@dataclass
class PerformanceMetrics:
    """Real-time performance monitoring."""
    total_requests: int = 0
    cache_hits: int = 0
    cache_misses: int = 0
    average_response_time: float = 0.0
    total_response_time: float = 0.0
    memory_usage: float = 0.0
    gpu_memory_usage: float = 0.0
    expert_switches: int = 0
    last_reset: float = field(default_factory=time.time)


class EnhancedChessInference:
    """Enhanced chess inference with optimized performance and reliability."""

    def __init__(self, config: Optional[InferenceConfig] = None):
        self.config = config or InferenceConfig()
        self.project_root = project_root

        # Core components
        self.tokenizer: Optional[AutoTokenizer] = None
        self.model: Optional[AutoModelForCausalLM] = None
        self.is_loaded = False

        # Expert management
        self.expert_adapters: Dict[str, str] = {}
        self.current_expert: str = "default"
        self.expert_cache: Dict[str, CacheEntry] = {}

        # Intelligent caching system
        self.response_cache: OrderedDict[str, CacheEntry] = OrderedDict()
        self.position_cache: Dict[str, List[CacheEntry]] = defaultdict(list)

        # Performance monitoring
        self.metrics = PerformanceMetrics()
        self.metrics_lock = threading.Lock()

        # Chess-specific optimizations
        self.chess_token_whitelist: Optional[set] = None
        self.move_pattern_cache: Dict[str, List[str]] = {}

        # Thread safety
        self.cache_lock = threading.Lock()
        self.model_lock = threading.Lock()

        logger.info("ğŸ”§ Enhanced Chess Inference initialized")

    def load_model(self, model_path: Optional[str] = None, adapter_path: Optional[str] = None) -> bool:
        """Load model with enhanced error handling and optimization."""
        try:
            # Default paths
            if not model_path:
                model_path = self.project_root / "models" / "unsloth-gemma-3-270m-it"
                # Find the actual model directory
                if model_path.exists():
                    snapshots = list(model_path.glob("snapshots/*"))
                    if snapshots:
                        model_path = max(snapshots, key=lambda p: p.stat().st_mtime)

            if not Path(model_path).exists():
                logger.error(f"Model path not found: {model_path}")
                return False

            logger.info(f"Loading tokenizer from {model_path}")
            self.tokenizer = AutoTokenizer.from_pretrained(
                str(model_path),
                local_files_only=True,
                trust_remote_code=True
            )

            logger.info(f"Loading model from {model_path}")
            self.model = AutoModelForCausalLM.from_pretrained(
                str(model_path),
                local_files_only=True,
                device_map="auto",
                attn_implementation="eager",
                torch_dtype=torch.float16
            )

            # Load adapter if specified
            if adapter_path and Path(adapter_path).exists():
                logger.info(f"Loading adapter from {adapter_path}")
                self.model = PeftModel.from_pretrained(
                    self.model,
                    str(adapter_path),
                    is_trainable=False
                )

            # Discover and load expert adapters
            self._discover_expert_adapters()

            # Configure pad/eos tokens
            if self.config.pad_token_id is None:
                self.config.pad_token_id = self.tokenizer.pad_token_id or self.tokenizer.eos_token_id
            if self.config.eos_token_id is None:
                self.config.eos_token_id = self.tokenizer.eos_token_id

            # Build chess-specific optimizations
            self._build_chess_optimizations()

            self.is_loaded = True
            logger.info("âœ… Model loaded successfully")
            return True

        except Exception as e:
            logger.error(f"âŒ Failed to load model: {e}")
            self.is_loaded = False
            return False

    def _discover_expert_adapters(self):
        """Discover and cache expert adapter paths."""
        checkpoints_dir = self.project_root / "checkpoints"

        # Look for expert-specific directories
        expert_patterns = {
            'uci': 'lora_uci',
            'tutor': 'lora_tutor',
            'director': 'lora_director',
            'curriculum': 'curriculum_training'
        }

        for expert_name, pattern in expert_patterns.items():
            expert_dir = checkpoints_dir / pattern
            if expert_dir.exists():
                # Find latest checkpoint
                checkpoints = list(expert_dir.glob("checkpoint-*"))
                if checkpoints:
                    latest_checkpoint = max(checkpoints, key=lambda p: p.stat().st_mtime)
                    self.expert_adapters[expert_name] = str(latest_checkpoint)
                    logger.info(f"Found {expert_name} adapter: {latest_checkpoint}")

    def _build_chess_optimizations(self):
        """Build chess-specific optimizations for faster inference."""
        if not self.tokenizer:
            return

        # Build whitelist of chess-related tokens
        chess_chars = set('abcdefgh12345678qrbnkQRBNKx+#=O-')
        chess_tokens = set()

        try:
            vocab = self.tokenizer.get_vocab()
            for token, token_id in vocab.items():
                # Include tokens that contain only chess characters
                if all(c in chess_chars for c in token.lower()):
                    chess_tokens.add(token_id)

            self.chess_token_whitelist = chess_tokens
            logger.info(f"Built chess token whitelist: {len(chess_tokens)} tokens")

        except Exception as e:
            logger.warning(f"Could not build chess token whitelist: {e}")

    def switch_expert(self, expert_name: str) -> bool:
        """Switch to a different expert adapter."""
        if not self.is_loaded or not hasattr(self.model, 'load_adapter'):
            return False

        if expert_name not in self.expert_adapters:
            logger.warning(f"Expert '{expert_name}' not found. Available: {list(self.expert_adapters.keys())}")
            return False

        try:
            adapter_path = self.expert_adapters[expert_name]
            self.model.load_adapter(adapter_path, adapter_name=expert_name)
            self.model.set_adapter(expert_name)
            self.current_expert = expert_name

            # Clear expert-specific cache
            with self.cache_lock:
                self.expert_cache.clear()

            with self.metrics_lock:
                self.metrics.expert_switches += 1

            logger.info(f"âœ… Switched to expert: {expert_name}")
            return True

        except Exception as e:
            logger.error(f"âŒ Failed to switch expert: {e}")
            return False

    def _generate_cache_key(self, prompt: str, config: InferenceConfig, expert: str) -> str:
        """Generate a unique cache key for the request."""
        config_str = f"{config.temperature}_{config.top_p}_{config.top_k}_{config.max_new_tokens}"
        key_components = [prompt, config_str, expert]
        key_string = "|".join(key_components)
        return hashlib.md5(key_string.encode()).hexdigest()

    def _get_cached_response(self, cache_key: str, fen: Optional[str] = None) -> Optional[str]:
        """Get cached response with position awareness."""
        if not self.config.cache_enabled:
            return None

        with self.cache_lock:
            # Check main cache
            if cache_key in self.response_cache:
                entry = self.response_cache[cache_key]
                entry.access_count += 1
                entry.timestamp = time.time()

                # Move to end (most recently used)
                self.response_cache.move_to_end(cache_key)

                with self.metrics_lock:
                    self.metrics.cache_hits += 1

                return entry.response

            # Check position-specific cache
            if fen:
                for entry in self.position_cache.get(fen, []):
                    if entry.expert_type == self.current_expert:
                        entry.access_count += 1
                        entry.timestamp = time.time()

                        with self.metrics_lock:
                            self.metrics.cache_hits += 1

                        return entry.response

            with self.metrics_lock:
                self.metrics.cache_misses += 1

            return None

    def _cache_response(self, cache_key: str, response: str, fen: Optional[str] = None,
                       move_uci: Optional[str] = None, confidence: float = 0.0):
        """Cache response with metadata."""
        if not self.config.cache_enabled:
            return

        with self.cache_lock:
            entry = CacheEntry(
                key=cache_key,
                response=response,
                fen=fen,
                move_uci=move_uci,
                expert_type=self.current_expert,
                confidence_score=confidence
            )

            # Add to main cache
            self.response_cache[cache_key] = entry
            if len(self.response_cache) > self.config.cache_max_size:
                # Remove least recently used
                self.response_cache.popitem(last=False)

            # Add to position cache
            if fen:
                self.position_cache[fen].append(entry)
                # Keep only recent entries per position
                if len(self.position_cache[fen]) > 10:
                    self.position_cache[fen] = self.position_cache[fen][-10:]

    def _extract_chess_info(self, prompt: str) -> Tuple[Optional[str], Optional[str]]:
        """Extract FEN and move information from prompt."""
        fen = None
        move_uci = None

        # Look for FEN in prompt
        fen_match = None
        import re
        fen_pattern = r'FEN:\s*([rnbqkbnrpnRNBQKBNRP12345678/]+)'
        match = re.search(fen_pattern, prompt)
        if match:
            fen_candidate = match.group(1)
            # Basic FEN validation
            if len(fen_candidate.split('/')) == 8:
                fen = fen_candidate

        return fen, move_uci

    def generate_response(self, prompt: str, mode: str = "tutor",
                         config: Optional[InferenceConfig] = None) -> Dict[str, Any]:
        """Generate enhanced response with full optimization."""
        start_time = time.time()

        if not self.is_loaded:
            return {
                "error": "Model not loaded",
                "response": "",
                "confidence": 0.0,
                "model_loaded": False
            }

        # Use provided config or default
        gen_config = config or self.config

        # Update metrics
        with self.metrics_lock:
            self.metrics.total_requests += 1

        try:
            # Generate cache key
            cache_key = self._generate_cache_key(prompt, gen_config, self.current_expert)

            # Extract chess information
            fen, move_uci = self._extract_chess_info(prompt)

            # Check cache first
            cached_response = self._get_cached_response(cache_key, fen)
            if cached_response:
                response_time = time.time() - start_time
                with self.metrics_lock:
                    self.metrics.total_response_time += response_time
                    self.metrics.average_response_time = (
                        self.metrics.total_response_time / self.metrics.total_requests
                    )

                return {
                    "response": cached_response,
                    "confidence": 0.9,  # High confidence for cached responses
                    "cached": True,
                    "expert": self.current_expert,
                    "response_time": response_time
                }

            # Generate new response
            with self.model_lock:
                response = self._generate_optimized(prompt, gen_config)

            # Cache the response
            self._cache_response(cache_key, response, fen, move_uci)

            # Calculate response time
            response_time = time.time() - start_time

            # Update performance metrics
            with self.metrics_lock:
                self.metrics.total_response_time += response_time
                self.metrics.average_response_time = (
                    self.metrics.total_response_time / self.metrics.total_requests
                )

            return {
                "response": response,
                "confidence": 0.8,  # Default confidence
                "cached": False,
                "expert": self.current_expert,
                "response_time": response_time
            }

        except Exception as e:
            logger.error(f"âŒ Generation failed: {e}")
            return {
                "error": str(e),
                "response": "",
                "confidence": 0.0,
                "cached": False,
                "expert": self.current_expert
            }

    def _generate_optimized(self, prompt: str, config: InferenceConfig) -> str:
        """Optimized generation with chess-specific enhancements."""
        try:
            # Tokenize input
            inputs = self.tokenizer(
                prompt,
                return_tensors="pt",
                truncation=True,
                max_length=2048
            ).to(self.model.device)

            # Configure generation
            generation_config = GenerationConfig(
                max_new_tokens=config.max_new_tokens,
                temperature=config.temperature,
                top_p=config.top_p,
                top_k=config.top_k,
                repetition_penalty=config.repetition_penalty,
                do_sample=config.do_sample,
                pad_token_id=config.pad_token_id,
                eos_token_id=config.eos_token_id,
                use_cache=True
            )

            # Generate response
            with torch.no_grad():
                outputs = self.model.generate(
                    **inputs,
                    generation_config=generation_config
                )

            # Decode response
            full_response = self.tokenizer.decode(outputs[0], skip_special_tokens=True)

            # Remove prompt from response
            if full_response.startswith(prompt):
                response = full_response[len(prompt):].strip()
            else:
                response = full_response.strip()

            return response

        except Exception as e:
            logger.error(f"âŒ Optimized generation failed: {e}")
            return ""

    def get_performance_stats(self) -> Dict[str, Any]:
        """Get current performance statistics."""
        with self.metrics_lock:
            cache_hit_rate = (
                self.metrics.cache_hits /
                (self.metrics.cache_hits + self.metrics.cache_misses)
                if (self.metrics.cache_hits + self.metrics.cache_misses) > 0 else 0
            )

            return {
                "total_requests": self.metrics.total_requests,
                "cache_hit_rate": cache_hit_rate,
                "average_response_time": self.metrics.average_response_time,
                "expert_switches": self.metrics.expert_switches,
                "memory_usage": self.metrics.memory_usage,
                "cache_size": len(self.response_cache),
                "current_expert": self.current_expert,
                "available_experts": list(self.expert_adapters.keys())
            }

    def clear_cache(self, expert_specific: bool = False):
        """Clear response cache."""
        with self.cache_lock:
            if expert_specific:
                # Clear only current expert's cache
                self.expert_cache.clear()
            else:
                # Clear all caches
                self.response_cache.clear()
                self.position_cache.clear()
                self.expert_cache.clear()

        logger.info("ğŸ§¹ Cache cleared")

    def preload_experts(self):
        """Preload all available expert adapters."""
        logger.info("ğŸ”„ Preloading expert adapters...")

        for expert_name, adapter_path in self.expert_adapters.items():
            try:
                logger.info(f"Loading {expert_name} adapter...")
                self.model.load_adapter(adapter_path, adapter_name=expert_name)
                logger.info(f"âœ… {expert_name} adapter loaded")
            except Exception as e:
                logger.warning(f"âŒ Failed to preload {expert_name}: {e}")

        logger.info("âœ… Expert preloading complete")


class ChessInferenceManager:
    """High-level manager for chess inference operations."""

    def __init__(self):
        self.inference_engine = EnhancedChessInference()
        self.executor = ThreadPoolExecutor(max_workers=4)

    def initialize(self) -> bool:
        """Initialize the inference system."""
        return self.inference_engine.load_model()

    def analyze_position(self, fen: str, mode: str = "tutor") -> Dict[str, Any]:
        """Analyze a chess position."""
        prompt = self._build_analysis_prompt(fen, mode)
        return self.inference_engine.generate_response(prompt, mode)

    def get_best_move(self, fen: str) -> Dict[str, Any]:
        """Get the best move for a position."""
        prompt = f"FEN: {fen}\nMove:\nStyle: balanced\nMode: Engine\nGenerate the best move in UCI format (e.g., e2e4). Respond with only the move."
        return self.inference_engine.generate_response(prompt, "engine")

    def _build_analysis_prompt(self, fen: str, mode: str) -> str:
        """Build analysis prompt based on mode."""
        base_prompt = f"FEN: {fen}\nQuestion: Analyze this position comprehensively and recommend the best move.\n\n"

        if mode == "tutor":
            base_prompt += """Consider:
1. Material balance and piece values
2. King safety (White king: h1, Black king: h8)
3. Piece activity and development
4. Pawn structure and weaknesses
5. Space control and initiative
6. Tactical opportunities and threats
7. Strategic goals and long-term plans

Provide a detailed analysis and end with your recommended move in UCI format."""
        elif mode == "engine":
            base_prompt += "Generate the best move in UCI format (e.g., e2e4). Respond with only the move."
        else:
            base_prompt += "Provide a comprehensive chess analysis."

        return base_prompt

    def switch_expert(self, expert_name: str) -> bool:
        """Switch to a different expert."""
        return self.inference_engine.switch_expert(expert_name)

    def get_stats(self) -> Dict[str, Any]:
        """Get system statistics."""
        return self.inference_engine.get_performance_stats()


# Global instance for easy access
_inference_manager: Optional[ChessInferenceManager] = None


def get_inference_manager() -> ChessInferenceManager:
    """Get global inference manager instance."""
    global _inference_manager
    if _inference_manager is None:
        _inference_manager = ChessInferenceManager()
    return _inference_manager


def initialize_inference() -> bool:
    """Initialize the global inference system."""
    manager = get_inference_manager()
    return manager.initialize()


def analyze_position(fen: str, mode: str = "tutor") -> Dict[str, Any]:
    """Analyze a chess position."""
    manager = get_inference_manager()
    return manager.analyze_position(fen, mode)


def get_best_move(fen: str) -> Dict[str, Any]:
    """Get the best move for a position."""
    manager = get_inference_manager()
    return manager.get_best_move(fen)


if __name__ == '__main__':
    # Test the enhanced inference system
    print("ğŸ¯ Testing Enhanced Chess Inference")

    if initialize_inference():
        print("âœ… Inference system initialized")

        # Test position analysis
        test_fen = "r1bqkbnr/pppp1ppp/2n5/4p3/4P3/5N2/PPPP1PPP/RNBQKB1R w KQkq - 2 3"
        result = analyze_position(test_fen, "tutor")
        print(f"Analysis result: {result}")

        # Test move generation
        move_result = get_best_move(test_fen)
        print(f"Best move: {move_result}")

        # Show stats
        stats = get_inference_manager().get_stats()
        print(f"Performance stats: {stats}")
    else:
        print("âŒ Failed to initialize inference system")
